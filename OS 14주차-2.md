# OS 14주차-2



OS 14주차-2

17)
디스크 스케줄링
핵심 :seek time을 줄이기 위해 재정렬(re-ordering)한다.
![OS 14주차-2](images/OS%2014주차-2.png)

응용프로그램 -> OS-> 디스크에 온 요청을 요청온(도착한) 순서대로 디스크는 서비스 하지 않고,
큐에 있는 요청순서를 바꾼다.
이유 : seek time을 최적화 하기 위해. seek time이 가장 큰 proportion을 차지하고 있기 때문

FCFS
요청순서를 바꾸지 않음. 순서대로 요청처리
seek time 측면에서 좋지 않을 수 있다.

queue 내의 넘버를 cylinder 넘버로 표현했다. 
실린더 : 스핀들로부터 거리가 동일한 트랙의 집합
![OS 14주차-2-1](images/OS%2014주차-2-1.png)

이유) 실린더 간의 이동일 경우에 seek이 필요한 경우
동일 실린더를 갖는 트랙에서 요청을 처리하는 것은 seek이 필요하지 않다. 헤드가 이미 가르키고 있기 때문

여기서 가장 안쪽실린더 넘버가 0, 가장 바깥족이 199라고 생각하면 되겠다

이와같은 경우, seek time이 커진다. 헤드가 그래프와 같이 계속 움직이니까.

비유)
대전 부산 대전 부산 대구 .. 이순서대로 요청이 들어왔을 때
FCFS는 저 순서 그대로 왔다갔다한다. -> 거리상 비효율적..

seek time은 seek distance에 비례하다. seek distance는 얼마나 많은 실린더를 거쳐가는지로 표현된다.

18)
SSTF
현재 헤드서부터 가장 짧은거리의 위치의 요청부터 처리

ex)
현재 헤드가 53번
-> 65번 처리
-> 67번 처리
…

-> seek distance가 훨씬 감소한다(236 cylinders)

문제)
특정 리퀘스트를 처리하지 못하는 starvation이 발생할 수 있다

현재 헤드랑 가까운 요청이 계속들어오면 헤드랑 거리가 먼 요청은 처리할 수 없는 상황이 올 수 있다.

+)SSTF는 최적이 아니다.
반례.
최초에 65를 먼저가지않고 37을 먼저가면 cylinder가 줄어든다(208 cylinders)

19)
SCAN(elevator algorithm)
스캐너를 기억하자.
스캐너 : 디스크를 안쪽부터 바깥쪽까지 헤드가 쭈욱 스캔하면서 지나가는 위치의 요청들 처리
마치 엘리베이터같음

starvation은 발생할 일이 없겠다.

스캔은 양방향으로 왔다갔다 하면서 한다.

20)
C-SCAN(one-direction)
![OS 14주차-2-2](images/OS%2014주차-2-2.png)

스캔을 한방향으로 만한다.

21)
Look 알고리즘

무조건 디스크 실린더의 끝까지 가지않고, 최소 또는 최대 넘버의 요청까지만 가고 방향전환

양방향

22)
C-Look

실린더 끝까지 않고, 최소 또는 최대 넘버의 요청까지 처리 후 방향전환

한방향

24)
Disk Management

디스크 포맷
= 파일시스템을 만들어 주는것. logical formatting
새로운 파일시스템을 입히기 위해 데이터를 다 지운다.

파일시스템은 partition 단위. 파티션 마다 별도의 파일시스템이 있을 수 있고, 파티션단위로 포맷가능하다(C드라이브, D드라이브 …)

로우레벨 포매팅
파일시스템을 입히는게 아니라 -> 섹터단위로 디스크를 구획화한다.
디스크를 리드/라이트 할 수 있도록 해주는 것
흔히 얘기하는 포맷은 아님. 제품이 처음 출시될 때 하는 것

부트블락은 생략

25)
swap-space

디스크는 디스크 일부를 swap-space로 쓴다 -> backing store
쫓겨난 페이지를 담을 수 있는 공간
![OS 14주차-2-3](images/OS%2014주차-2-3.png)

스왑 스페이스 관리

1. 일반 파일시스템 아래서 관리
2. 별도의 파티션을 두어서 관리, 각자의 스왑스페이스 매니저를 두어
	raw partition -> 포맷이 안된 파티션
	cooked partition -> 포맷이 된 파티션

보통 2번사용

26)
swap map을 써서 관리하는 방식(스왑매니저).

swap area : 쫓겨난 스왑페이지가 저장된 영역
페이지 슬롯마다 사용하는 프로세스의 개수를 적어놓음
비어있는 슬롯 ->2번, 4번
3번은 3개의 프로세스가 해당 페이지를 씀 -> 즉 3번 페이지가 shared 영역이다.

27)
RAID Structure

Redundant Array of Inexpensive(Independent) Disk
(요즘은 Independent를 더 많이 쓴다)
독립적인 디스크의 중복된 배열

디스크를 여러개 둔다. array처럼.
여러 디스크를 병렬로 사용하는것.

왜?
- 디스크를 여려개 쓰면 용량이 커진다.
- 디스크를 많이 쓰면 Throughput 향샹 예를들어 디스크 1개당 1초에 1000개 io처리가능 -> 10개를 쓰면 10000개 가능(이상적인 계산이지만)

데이터를 중복해서 저장하는 이유.
- Fault-Tolerance의 제공. Fault가 발생하더라도 감내가 가능하도록. 디스크가 고장나면 데이터가 날라가니까. 이 때 싱글디스크보다 RAID같이 여러 디스크면 fault rate(고장날확률)가 올라간다. -> 당연..하겠다.  디스크가 고장나면 복구해줘야한다. 1. Mirroring 데이터를 다른 곳에 복제해서 저장. -> 용량이 2배필요 -> 용량이 좀 낭비겠다.. 2. Parity 용량을 save하면서 고장이 발생하더라도 복구가 가능하다

28)
Parity의 기본아이디어는 XOR 연산이다.
![OS 14주차-2-4](images/OS%2014주차-2-4.png)

남아있는 데이터들과 parity를 XOR하면 데이터가 복구 가능하다.
디스크가 몇개던 가능.

미러링 기법의 데이터 용량과 비교해보자.

striping size : (?) 못들음

![OS 14주차-2-5](images/OS%2014주차-2-5.png)

미러링은 N개의 디스크가 있다면 N개의 용량이 더필요.

패리티 기법은 추가적인 용량이 : 1/string size * 디스크 개수

=> 용량은 줄이면서 복구가능
그러나 2개이상의 디스크 오류는 복구가 불가능

29)
A라는 파일을 하나의 디스크에 저장할 수 도 있고,
파일을 쪼개서 여러 디스크에 걸쳐서 저장할 수 있다 -> striping 기법

비트레벨 스타라이핑 -> 비트레벨까지 쪼갬. 엄청 잘게 쪼갠다. 진짜 비트레벨까지 쪼개는건 불가능하고, 그냥 잘게 쪼개자. fine-granulanrity
블락레벨 스트라이핑 -> 상대적으로 큼직하게 쪼개자. coarse
	=> 스트라이핑 사이즈(스트라이핑 단위) 차이
![OS 14주차-2-6](images/OS%2014주차-2-6.png)

A라는 파일을 처리하려면 모든 디스크가 synchronize되어서 working한다.

30)
fault-tolerance

디스크 개수 증가 -> fault-rate 증가

Mean time to failure(MTTF) : failure가 일어날 때까지 평균시간

n개의 디스크에 대해 MTTF를 계산하려면 디스크의 개수를 나눠줘야한다.
-> 디스크가 100개면 41일에 한번..?

디스크 고장은 data loss이기 때문에 치명적.

방법은 redundacny이다.
mirroring 또는 parity

디스크가 고장이나면, 
1. 고장난 디스크를 교체해야한다.(replacement)
2. 교체한 디스크에 데이터를 넣어줘야한다(rebuilding)
=> 이 시간을 Mean time to repair

리빌딩시간은 mirroring이 길까, parity가 길까?
parity가 길다. -> 계산을 해야하니까
mirroring은 그냥 copy and paste하면 됨

언제 data loss가 발생할까?
![OS 14주차-2-7](images/OS%2014주차-2-7.png)

디스크 1개가 고장이나서 고치고있는데, 다른 디스크가 고장날때
parity는 2개이상의 디스크 고장이나면 복구가 불가..

저 사이에, 즉 failure가 나고 repair가 되기 전까지 고장이 날확률?
-> 57000년. 이상적이긴 하지만 여튼..

31)
RAID levels

레이드0
non-redundant striping
strinping만 쓰고, redundancy는 X
-> 복구 불가능..

레이드1
미러링 사용. striping은 X

레이드2
고려할필요없다. 현재 쓰이지않음

레이드3
parity 디스크를 두고 비트레벨로 striping, 잘게 쪼갠다.

레이드4
parity 디스크를 두고 블락레벨 striping 사용

parity 디스크는 평상시에 쓰지않고 복구에만 사용한다 -> 사용률이 떨어진다.
이를 해결하기위해,
레이드 5
parity를 분산시키고, 블락레벨로 스트라이핑
분산해서 저장시키면 모든 디스크가 동시에 사용된다.
-> disk band width를 효과적으로 쓸 수 있다

레이드 6
parity 디스크 1개를 쓰면,
디스크가 2개이상고장나면 복구 불가능 했다

2개이상의 disk failure에도 복구가능하도록
P+Q Redundancy 테크닉 사용

32)
디스크 교체 시 hot spare을 둔다.
스페어 디스크를 의미.
고장이 나면 스페어 디스크에 rebuilding이 이루어진다.

rebuilding : 이와 같은 디스크에 저장된 데이터를 recover하는 과정 

33)
레이드의 한가지 문제점

small-write problem

라이트 연산을 한다.
두번째 디스크를 업데이트한다.

레이드 같은 경우는 parity 디스크도 추가적으로 업데이트 해야한다.
pariy를 업데이트 하기위해서는 모든 디스크를 xor하기 위해, 나머지 디스크를 read하는 연산이 필요하다.
-> B에 저장되어있는 데이터 하나 작게 바꿨을 뿐인데, 다른 디스크도 모두 읽어야하는 추가적인 read 연산이 필요

